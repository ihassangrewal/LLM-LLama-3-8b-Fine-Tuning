{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages and clone the unsloth's Github Repo\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "import nltk\n",
    "from datasets import load_metric\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration settings\n",
    "max_seq_len = 4096\n",
    "data_type = None  # Auto-detection\n",
    "use_quantization = True  # 4-bit quantization for reduced memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model name\n",
    "model_name_4bit = \"unsloth/llama-3-8b-bnb-4bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name_4bit,\n",
    "    max_seq_length=max_seq_len,\n",
    "    dtype=data_type,\n",
    "    load_in_4bit=use_quantization,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure PEFT model\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  # Optimized for 0\n",
    "    bias=\"none\",    # Optimized for \"none\"\n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  # Support for rank stabilized LoRA\n",
    "    loftq_config=None, # LoftQ support\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON datasets\n",
    "def load_json_data(file_path):\n",
    "    with open(file_path, \"r\", encoding='utf-8-sig') as f:  # Use 'utf-8-sig' to handle BOM\n",
    "        return json.load(f)\n",
    "\n",
    "auto_eval_topics_tree = load_json_data(\"/content/2022_automatic_evaluation_topics_tree_v1.0.json\")\n",
    "additional_data = load_json_data(\"/content/2022_mixed_initiative_question_answer_pool.json\")\n",
    "\n",
    "# Extract and process data\n",
    "def process_data(topics_tree):\n",
    "    instructions, inputs, responses = [], [], []\n",
    "    for topic in topics_tree:\n",
    "        turns = topic['turn']\n",
    "        turn_dict = {turn['number']: turn for turn in turns}\n",
    "        for turn in turns:\n",
    "            if turn['participant'] == 'User':\n",
    "                parent_turn_id = turn.get('parent')\n",
    "                parent_turn = turn_dict.get(parent_turn_id, {})\n",
    "                instruction = parent_turn.get('automatic_rewritten_utterance', parent_turn.get('utterance', ''))\n",
    "                input_text = turn.get('automatic_rewritten_utterance', turn.get('utterance', ''))\n",
    "                response_turn = next((item for item in turns if item.get('parent') == turn['number'] and item['participant'] == 'System'), {})\n",
    "                response_text = response_turn.get('response', '')\n",
    "                instructions.append(instruction)\n",
    "                inputs.append(input_text)\n",
    "                responses.append(response_text)\n",
    "    return instructions, inputs, responses\n",
    "\n",
    "instructions, inputs, responses = process_data(auto_eval_topics_tree)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'instruction': instructions, 'input': inputs, 'response': responses})\n",
    "df['instruction'] = df.apply(lambda row: row['input'] if row['instruction'] == '' else row['instruction'], axis=1)\n",
    "\n",
    "# Combine and save data\n",
    "combined_data = df.to_dict(orient='records') + additional_data\n",
    "output_file = \"/content/combined_instruction_input_response_data.json\"\n",
    "with open(output_file, \"w\", encoding='utf-8-sig') as f:  # Use 'utf-8-sig' to handle BOM\n",
    "    json.dump(combined_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Load combined data and remove duplicates\n",
    "combined_data = load_json_data(output_file)\n",
    "df = pd.DataFrame(combined_data)\n",
    "df_unique = df.drop_duplicates(subset='instruction')\n",
    "unique_data = df_unique.to_dict(orient='records')\n",
    "cleaned_output_file = \"/content/cleaned_combined_instruction_input_response_data.json\"\n",
    "with open(cleaned_output_file, \"w\", encoding='utf-8-sig') as f:  # Use 'utf-8-sig' to handle BOM\n",
    "    json.dump(unique_data, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display last few entries\n",
    "def display_last_entries(data, num_entries=5):\n",
    "    for entry in data[-num_entries:]:\n",
    "        print(json.dumps(entry, indent=4))\n",
    "\n",
    "display_last_entries(combined_data, num_entries=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace Dataset and prepare data\n",
    "def prepare_dataset(file_path):\n",
    "    with open(file_path, encoding='utf-8-sig') as f:\n",
    "        data = json.load(f)\n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "dataset = prepare_dataset(cleaned_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting prompt function\n",
    "prompt_template = \"\"\"Kindly have a look at the instruction and give response.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def format_prompts(data):\n",
    "    return {\"text\": [prompt_template.format(inst, inp, resp) + EOS_TOKEN for inst, inp, resp in zip(data['instruction'], data['input'], data['response'])]}\n",
    "\n",
    "dataset = dataset.map(format_prompts, batched=True)\n",
    "\n",
    "# Split dataset\n",
    "df = dataset.to_pandas()\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Inference Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    prompt_template.format(\n",
    "        \"What is COP26?\", # instruction\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "decoded_outputs= tokenizer.batch_decode(outputs)\n",
    "for output in decoded_outputs:\n",
    "    print(output.replace(\"\\\\n\", \"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    max_steps=24,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    output_dir=\"outputs\",\n",
    "    gradient_checkpointing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and run the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_len,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "trainer_stats = trainer.train()\n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n",
    "print(f\"Time taken for this instruction to run is: {time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime'] / 60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Inference After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "# Define the prompt with placeholders\n",
    "prompt_template = \"\"\"Kindly have a look at the instruction and give response.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "# Define the instruction, input, and empty response\n",
    "instruction = \"What is COP26?\"\n",
    "input_text = \"What is COP26?\"\n",
    "response = \"\"\n",
    "\n",
    "# Format the prompt\n",
    "formatted_prompt = prompt_template.format(instruction, input_text, response)\n",
    "\n",
    "# Tokenize the inputs\n",
    "inputs = tokenizer([formatted_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Measure the time taken for the model to generate the output\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate the outputs\n",
    "outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
    "\n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "# Decode the outputs\n",
    "decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "\n",
    "# Print the time taken\n",
    "print(f\"Time taken for this instruction to run is : {time_taken:.2f} seconds\")\n",
    "\n",
    "# Print the decoded outputs with proper newlines\n",
    "for output in decoded_outputs:\n",
    "    print(output.replace(\"\\\\n\", \"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and evaluate with BLEU and ROUGE metrics\n",
    "!pip install rouge_score\n",
    "!pip install datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure nltk data is downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "dataset = test_dataset\n",
    "\n",
    "limited_dataset = dataset.select(range(100))\n",
    "\n",
    "# Function to generate predictions with attention mask and pad token id\n",
    "def generate_predictions(input_text):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    outputs = model.generate(inputs.input_ids, attention_mask=attention_mask, max_length=128, pad_token_id=tokenizer.eos_token_id)\n",
    "    predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return predicted_text\n",
    "\n",
    "# Prepare lists for predictions and references\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "# Generate predictions for the dataset\n",
    "for example in limited_dataset:\n",
    "    input_text = example['input']\n",
    "    reference_text = example['response']\n",
    "    predicted_text = generate_predictions(input_text)\n",
    "    predictions.append(predicted_text)\n",
    "    references.append([reference_text])\n",
    "\n",
    "# Tokenize predictions and references for BLEU\n",
    "predictions_tokenized = [nltk.word_tokenize(pred) for pred in predictions]\n",
    "references_tokenized = [[nltk.word_tokenize(ref) for ref in ref_group] for ref_group in references]\n",
    "\n",
    "# Load evaluation metrics\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "bleu_metric = load_metric(\"bleu\")\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_score = bleu_metric.compute(predictions=predictions_tokenized, references=references_tokenized)\n",
    "\n",
    "# Calculate ROUGE score\n",
    "rouge_score = rouge_metric.compute(predictions=predictions, references=[[\" \".join(ref) for ref in ref_group] for ref_group in references])\n",
    "\n",
    "# Extract mid F1 scores for ROUGE\n",
    "rouge1_mid_f1 = rouge_score['rouge1'].mid.fmeasure\n",
    "rouge2_mid_f1 = rouge_score['rouge2'].mid.fmeasure\n",
    "rougeL_mid_f1 = rouge_score['rougeL'].mid.fmeasure\n",
    "rougeLsum_mid_f1 = rouge_score['rougeLsum'].mid.fmeasure\n",
    "\n",
    "# Calculate the average ROUGE score\n",
    "average_rouge = (rouge1_mid_f1 + rouge2_mid_f1 + rougeL_mid_f1 + rougeLsum_mid_f1) / 4\n",
    "\n",
    "# Extract BLEU score\n",
    "average_bleu = bleu_score['bleu']\n",
    "\n",
    "print(f\"Average BLEU Score: {average_bleu}\")\n",
    "print(f\"Average ROUGE Score: {average_rouge}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the scores\n",
    "labels = ['BLEU', 'ROUGE']\n",
    "scores = [average_bleu, average_rouge]\n",
    "\n",
    "x = range(len(labels))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x, scores, width=0.4, label='Model Scores', align='center')\n",
    "\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Llama 3 8b -Average BLEU and ROUGE Scores')\n",
    "plt.xticks(x, labels)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
